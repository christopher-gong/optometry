{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from importHelpers.response import *\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean\n",
    "##### We import and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(r'data\\\\191126P2_ROIAnnotationSummary_200218.xlsx')\n",
    "df = pd.read_excel(xls, 'Annotation_Summary')\n",
    "df = df[['Flash', '2P ROI', 'RBPMS', 'Syt10+', 'Syt6+', 'CAVIII', 'ChAT', 'Satb2', 'MEIS', 'CalR']]\n",
    "df = df.dropna(axis = 0, subset = [\"2P ROI\"])\n",
    "df = df[df['2P ROI'].apply(lambda x: str(x).isdigit())]\n",
    "df = df.astype({\"2P ROI\": int})\n",
    "for col in ['Syt10+', 'Syt6+', 'CAVIII', 'ChAT', 'Satb2', 'MEIS', 'CalR']:\n",
    "    df[col] = df[col].apply(lambda x: int(not pd.isna(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(df.T)\n",
    "def name_merge(x):\n",
    "    p = [str(i[l[x]]) for _, i in df.loc[[l[x]]].to_dict().items()]\n",
    "    return p[0] + '_wave_' + str(p[1])\n",
    "name_merge(0)\n",
    "\n",
    "def uniquer(x):\n",
    "    return \"\".join([str(i[l[x]]) for _, i in df.loc[[l[x]]].to_dict().items()][2:])\n",
    "\n",
    "d = {}\n",
    "c = 0\n",
    "z = []\n",
    "for i in range(df.shape[0]):\n",
    "    u = uniquer(i)\n",
    "    if u not in d.keys():\n",
    "        d[u] = c\n",
    "        c += 1\n",
    "    z.append(d[u])\n",
    "df.insert(10, \"Class\", z)\n",
    "\n",
    "s = []\n",
    "for i in range(df.shape[0]):\n",
    "    s.append(name_merge(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine\n",
    "##### We combine our data into one large sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on sheet 2 of 8\n",
      "Working on sheet 3 of 8\n",
      "Working on sheet 4 of 8\n",
      "Working on sheet 5 of 8\n",
      "Working on sheet 6 of 8\n",
      "Working on sheet 7 of 8\n",
      "Working on sheet 8 of 8\n",
      "Sheet combination complete.\n"
     ]
    }
   ],
   "source": [
    "# FILENAME\n",
    "xlsx_filename = \"data\\\\191126P2PhysData_withlabels.xlsx\"\n",
    "excel = pd.ExcelFile(xlsx_filename)\n",
    "\n",
    "def renamer(sheet, ind):\n",
    "    l = lambda name: str(ind) + '_' + name\n",
    "    sheet = sheet.rename(index = l)\n",
    "    return sheet\n",
    "i = 0\n",
    "new_sheetnames = ['Flash_40', 'Flash_52', 'Flash_56', 'Flash_58', 'Flash_60', 'Flash_66', 'Flash_68', 'Flash_46']\n",
    "total = renamer(pd.read_excel(xlsx_filename,sheet_name=excel.sheet_names[i], header=0).T, new_sheetnames[0])\n",
    "for i in range(1, len(excel.sheet_names)):\n",
    "    print('Working on sheet ' + str(i + 1) + ' of ' + str(len(excel.sheet_names)))\n",
    "    total = total.append(renamer(pd.read_excel(xlsx_filename,sheet_name=excel.sheet_names[i], header=0).T, new_sheetnames[i]))\n",
    "print(\"Sheet combination complete.\")\n",
    "n = total\n",
    "\n",
    "def getClassByName(name):\n",
    "    return z[s.index(name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = n[[i in s for i in n.index]]\n",
    "n_class = []\n",
    "for name in list(n.index):\n",
    "    n_class.append(getClassByName(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(initial):\n",
    "    # remove and subtract baseline\n",
    "    # c = frameToSecDF(initial.sub(initial['baseline'], axis = 'rows').drop('baseline', axis = 1).T)\n",
    "    # drop 70\n",
    "    c = initial\n",
    "    a = [a - b > 70 for a, b in zip(list(c.max(axis = 1)), list(c.min(axis= 0)))]\n",
    "    dropped = []\n",
    "    for i in range(len(a)):\n",
    "        if not a[i]:\n",
    "            dropped.append(list(c.T)[i])\n",
    "    c = c.drop(dropped, axis = 0)\n",
    "    # -1 1 scale\n",
    "    last = c[c.columns[-15:]]\n",
    "    last = last.mean(axis=1)\n",
    "    ne = c.sub(last, axis = 0)\n",
    "    n_one = ne.div(ne.abs().max(axis = 1), axis = 0)\n",
    "    return n_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#n = df\n",
    "pca = PCA(n_components=30)\n",
    "principalComponents = pca.fit_transform(n)\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "pca_n = pd.DataFrame(data = pca.inverse_transform(principalComponents))\n",
    "pca_n = pca_n.rename(index={a:b for a,b in zip(range(len(list(n.T))),list(n.T))}, columns={a:b for a,b in zip(range(len(list(n))),list(n))})\n",
    "# comment next line for no PCA\n",
    "next_n = n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster\n",
    "##### We cluster our data and check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN with your params found:\n",
      "12 classified labels\n",
      "298 unclassified points out of 603\n"
     ]
    }
   ],
   "source": [
    "db = DBSCAN(eps=3, min_samples=2).fit(next_n)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "dlabels = db.labels_\n",
    "print(\"DBSCAN with your params found:\")\n",
    "print(str(max(dlabels + 1)) + \" classified labels\")\n",
    "print(str(list(dlabels).count(-1)) + ' unclassified points out of ' + str(len(dlabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(dlabels, n_class):    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(n_class)):\n",
    "        for j in range(i + 1, len(n_class)):\n",
    "            if (dlabels[i] == -1):\n",
    "                continue\n",
    "            if (dlabels[i] == dlabels[j]):\n",
    "                if (n_class[i] == n_class[j]):\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    print(correct / total)\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5713700803521162\n"
     ]
    }
   ],
   "source": [
    "c, t = accuracy(dlabels, n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5713700803521162"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c/t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
